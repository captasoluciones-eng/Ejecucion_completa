name: Carga CreditosActivos10 a BigQuery

on:
  workflow_dispatch: # Permite ejecutarlo manualmente
  schedule:
    - cron: "0 6 * * *" # Se ejecuta todos los d√≠as a las 6:00 UTC (aj√∫stalo si quieres)

jobs:
  upload-to-bigquery:
    runs-on: ubuntu-latest

    steps:
      - name: Configurar Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.12'

      - name: Instalar dependencias
        run: |
          python -m pip install --upgrade pip
          pip install pandas gdown google-cloud-bigquery

      - name: Guardar credenciales GCP
        run: |
          echo "${{ secrets.GCP_CREDENTIALS }}" > $HOME/gcp_credentials.json
        # Aseg√∫rate de que GCP_CREDENTIALS sea tu JSON completo en GitHub Secrets

      - name: Ejecutar script CreditosActivos10
        env:
          GOOGLE_APPLICATION_CREDENTIALS: $HOME/gcp_credentials.json
        run: |
          python3 << 'EOF'
import os
import pandas as pd
import gdown
from io import BytesIO
from google.cloud import bigquery

# Ruta de credenciales
cred_path = os.path.expanduser(os.getenv("GOOGLE_APPLICATION_CREDENTIALS"))
print(f"üîë Usando credenciales: {cred_path}")

# Crear cliente BigQuery expl√≠citamente con JSON
client = bigquery.Client.from_service_account_json(cred_path)

# Descargar CSV en memoria
file_id = "1Tu1__f_w-s7V2UCyP5RFVNRMFtnJ0zfE"
url_csv = f"https://drive.google.com/uc?id={file_id}"
print("üì• Descargando CSV...")
csv_bytes = gdown.download(url_csv, output=None, quiet=False)
csv_buffer = BytesIO(csv_bytes)

# Leer CSV
df = pd.read_csv(csv_buffer, sep=",", on_bad_lines="skip", low_memory=False, encoding="utf-8-sig")
df.columns = df.columns.str.strip().str.replace(" ", "_").str.replace(r"[^a-zA-Z0-9_]", "", regex=True)
print(f"‚úÖ CSV le√≠do correctamente. Filas: {len(df)} | Columnas: {len(df.columns)}")

# Configurar BigQuery
proyecto_bq = "lookerstudio-consolidacion"
dataset_bq = "DatosLooker_USC"
tabla_bq = "Full"
tabla_destino = f"{proyecto_bq}.{dataset_bq}.{tabla_bq}"

# Subir datos
job_config = bigquery.LoadJobConfig(write_disposition="WRITE_TRUNCATE", autodetect=True)
job = client.load_table_from_dataframe(df, tabla_destino, job_config=job_config)
job.result()
print(f"‚úÖ Datos subidos correctamente a {tabla_destino}")
EOF
