name: Cargar CreditosActivos10 a BigQuery

on:
  workflow_dispatch:

jobs:
  upload_to_bigquery:
    runs-on: ubuntu-latest
    steps:

      # 1Ô∏è‚É£ Clonar el repositorio
      - name: Checkout repo
        uses: actions/checkout@v3

      # 2Ô∏è‚É£ Configurar Python
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.12'

      # 3Ô∏è‚É£ Instalar dependencias
      - name: Instalar librer√≠as
        run: |
          python -m pip install --upgrade pip
          pip install pandas gdown google-cloud-bigquery

      # 4Ô∏è‚É£ Guardar credenciales de GCP desde secretos
      - name: Guardar credenciales GCP
        run: |
          echo "${{ secrets.GCP_CREDENTIALS }}" > $HOME/gcp_credentials.json
          echo "‚úÖ Credenciales guardadas en $HOME/gcp_credentials.json"

      # 5Ô∏è‚É£ Ejecutar script en memoria sin guardar CSV
      - name: Ejecutar CreditosActivos10
        env:
          GOOGLE_APPLICATION_CREDENTIALS: $HOME/gcp_credentials.json
        run: |
          python3 << 'EOF'
          import os
          import pandas as pd
          import gdown
          from io import BytesIO
          from google.cloud import bigquery

          print(f"üîë Usando credenciales: {os.getenv('GOOGLE_APPLICATION_CREDENTIALS')}")

          # Cliente BigQuery
          client = bigquery.Client()

          # Descargar CSV directamente en memoria
          file_id = "1Tu1__f_w-s7V2UCyP5RFVNRMFtnJ0zfE"
          url_csv = f"https://drive.google.com/uc?id={file_id}"
          print("üì• Descargando CSV...")
          csv_bytes = gdown.download(url_csv, output=None, quiet=False)
          csv_buffer = BytesIO(csv_bytes)

          # Leer CSV
          df = pd.read_csv(csv_buffer, sep=",", on_bad_lines="skip", low_memory=False, encoding="utf-8-sig")
          df.columns = df.columns.str.strip().str.replace(" ", "_").str.replace(r"[^a-zA-Z0-9_]", "", regex=True)
          print(f"‚úÖ CSV le√≠do correctamente. Filas: {len(df)} | Columnas: {len(df.columns)}")

          # Configuraci√≥n BigQuery
          proyecto_bq = "lookerstudio-consolidacion"
          dataset_bq = "DatosLooker_USC"
          tabla_bq = "Full"
          tabla_destino = f"{proyecto_bq}.{dataset_bq}.{tabla_bq}"

          job_config = bigquery.LoadJobConfig(write_disposition="WRITE_TRUNCATE", autodetect=True)

          # Subir a BigQuery
          job = client.load_table_from_dataframe(df, tabla_destino, job_config=job_config)
          job.result()
          print(f"‚úÖ Datos subidos correctamente a {tabla_destino}")
          EOF
