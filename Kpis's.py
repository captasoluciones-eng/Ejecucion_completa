{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMBvIWnvevLGH7f/3H+JyvB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fBxtTTIkDWSd","executionInfo":{"status":"ok","timestamp":1758823661351,"user_tz":420,"elapsed":22279,"user":{"displayName":"CAPTA SOLUCIONES","userId":"17177378079453460348"}},"outputId":"3b2dc458-451d-4a36-be3c-bc93c2b19c49"},"outputs":[{"output_type":"stream","name":"stdout","text":["ðŸ”¹ Iniciando proyecto mi rey: Carga Detallado de canje a BigQuery\n","Descargando CSV...\n"]},{"output_type":"stream","name":"stderr","text":["Downloading...\n","From: https://drive.google.com/uc?id=1BWMw72Jt2XEzoqXzDTOBUH6wtUbeF3Q2\n","To: /content/TodosLosDatosKpiConsolidados.csv\n","100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 219k/219k [00:00<00:00, 57.6MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Leyendo CSV...\n","CSV leÃ­do correctamente. Filas: 1379  Columnas: 17\n","Subiendo datos a BigQuery...\n","Datos subidos correctamente a lookerstudio-consolidacion.DatosLooker_USC.Kpis\n"]}],"source":["from google.colab import auth\n","auth.authenticate_user()\n","\n","import pandas as pd\n","import gdown\n","from google.cloud import bigquery\n","\n","\n","\n","print(\"ðŸ”¹ Iniciando proyecto mi rey: Carga Detallado de canje a BigQuery\")\n","\n","# -----------------------------\n","# ConfiguraciÃ³n\n","# -----------------------------\n","\n","# ID de archivo de Google Drive\n","file_id = \"1BWMw72Jt2XEzoqXzDTOBUH6wtUbeF3Q2\"\n","url_csv = f\"https://drive.google.com/uc?id={file_id}\"\n","archivo_csv = \"TodosLosDatosKpiConsolidados.csv\"\n","\n","proyecto_bq = \"lookerstudio-consolidacion\"\n","dataset_bq = \"DatosLooker_USC\"\n","tabla_bq = \"Kpis\"\n","\n","# -----------------------------\n","# 1. Descargar CSV\n","# -----------------------------\n","print(\"Descargando CSV...\")\n","gdown.download(url_csv, archivo_csv, quiet=False)\n","\n","# -----------------------------\n","# 2. Leer CSV (forzando separador)\n","# -----------------------------\n","print(\"Leyendo CSV...\")\n","try:\n","    df = pd.read_csv(archivo_csv, sep=\",\", on_bad_lines='skip', low_memory=False, encoding='utf-8-sig')\n","    print(f\"CSV leÃ­do correctamente. Filas: {len(df)}  Columnas: {len(df.columns)}\")\n","except Exception as e:\n","    print(\"Error al leer el CSV:\", e)\n","    exit()\n","\n","# -----------------------------\n","# 3. Limpiar nombres de columnas para BigQuery\n","# -----------------------------\n","df.columns = (\n","    df.columns\n","    .str.strip()                           # quitar espacios\n","    .str.replace(\" \", \"_\")                 # espacios por guiones bajos\n","    .str.replace(r\"[^a-zA-Z0-9_]\", \"\", regex=True)  # quitar caracteres no vÃ¡lidos\n",")\n","\n","# -----------------------------\n","# 4. Subir a BigQuery\n","# -----------------------------\n","print(\"Subiendo datos a BigQuery...\")\n","client = bigquery.Client(project=proyecto_bq)\n","\n","tabla_destino = f\"{proyecto_bq}.{dataset_bq}.{tabla_bq}\"\n","\n","job_config = bigquery.LoadJobConfig(\n","    write_disposition=\"WRITE_TRUNCATE\",\n","    autodetect=True\n",")\n","\n","try:\n","    job = client.load_table_from_dataframe(df, tabla_destino, job_config=job_config)\n","    job.result()\n","    print(f\"Datos subidos correctamente a {tabla_destino}\")\n","except Exception as e:\n","    print(\"Error al subir a BigQuery:\", e)\n"]}]}